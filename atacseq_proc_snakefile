import glob

# Config file
configfile: "config.yml"

#working directory
#workdir: config['workdir']

# Paths
datadir = config["datadir"]

resdir= config["resdir"]

filtbamdir = resdir + "bam_filtered/"
filtbamdir_dup = resdir + "bam_filtered_noDupRm/"


logdir = config["logdir"]
rawbamdirstats= logdir + "bam_raw_stats/"
filtbamdirstats= logdir + "bam_filt_stats/"

qcdir= resdir + "qc/"
xcordir= qcdir + "xcor/"
fingerprintdir = qcdir + "deepTools_fingerprint/"
corrdir = qcdir + "deepTools_correlationHM/"


deeptoolsdir = resdir + "deepTools/"
covdir1x = deeptoolsdir + "coverage1x/"
hmdir = deeptoolsdir + "heatmaps_clust/"
multiBamSummarydir= deeptoolsdir + "multiBamSummary/"

tmpdir= os.getenv('TMPDIR', default='/tmp')

tempdir= resdir + "temporary/"

# Get samples & file bases in the fastq data directory
bam_base = glob.glob(datadir + "*.bam")                  
bam_base = [s.replace('.bam', '') for s in bam_base]
bam_base = [s.replace(datadir, '') for s in bam_base]


#print(bam_base)


#####################
## TMPDIR

# in this version $TMPDIR is used in the shell code chunks
#####################


rule all:
    input:
        resdir + "rulegraph.png",
        resdir + "dag.png",
        expand(filtbamdirstats + "{id}.filt.stats", id=bam_base)


rule filter_bam:
    input:
        datadir + "{id}.bam"
    output:
        filt= filtbamdir + "{id}.filt.bam",
        log_st_flagstat= rawbamdirstats + "{id}.raw.flagstat",
        log_st_idxstats= rawbamdirstats + "{id}.raw.idxstats",
        log_st_stats= rawbamdirstats + "{id}.raw.stats",
        log_picard= logdir + "{id}.MarkDuplicates.metrics.txt",
        log_Mt= logdir + "{id}.filt.Mt.log",
        log_blcklst= logdir + "{id}.filt.blacklist.log",
        nonduprm= filtbamdir_dup +  "{id}.filt.NOduprm.bam",
        nonduprm_idx= filtbamdir_dup +  "{id}.filt.NOduprm.bam.bai"
    shell:
        """
        module load bioinfo-tools
        module load samtools/1.9
        module load picard/2.20.4
        
        TMPDIR="${{TMPDIR:-/tmp}}"
        PICARD_HOME="${{PICARD_HOME}}"
    
        echo "sort, index and compute stats for bam file"

        samtools sort -T $TMPDIR/{wildcards.id}.tmp -o $TMPDIR/{wildcards.id}.sorted.bam {input}
        samtools index $TMPDIR/{wildcards.id}.sorted.bam
        
        samtools flagstat $TMPDIR/{wildcards.id}.sorted.bam > {output.log_st_flagstat}
        samtools idxstats $TMPDIR/{wildcards.id}.sorted.bam > {output.log_st_idxstats}
        samtools stats $TMPDIR/{wildcards.id}.sorted.bam > {output.log_st_stats}
    
        
        echo "aln quality filter"

        samtools view -q 1 -hbo $TMPDIR/{wildcards.id}.sorted.q.bam $TMPDIR/{wildcards.id}.sorted.bam

        echo "mark duplicates"

        java -Xmx6G -jar $PICARD_HOME/picard.jar MarkDuplicates I=$TMPDIR/{wildcards.id}.sorted.q.bam \
            O=$TMPDIR/{wildcards.id}.dedup.bam \
            M={output.log_picard} VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=false \
            ASSUME_SORTED=true TMP_DIR=$TMPDIR

        echo "index and filter reads mapped to Mt; and compute stats for bam file"

        # rm mt and blacklist
        samtools index $TMPDIR/{wildcards.id}.dedup.bam
        samtools view -b -L {config[mt]} -U $TMPDIR/{wildcards.id}.filtMT.bam -o $TMPDIR/{wildcards.id}.filtMT.in.bam $TMPDIR/{wildcards.id}.dedup.bam
        samtools index $TMPDIR/{wildcards.id}.filtMT.bam
        samtools index $TMPDIR/{wildcards.id}.filtMT.in.bam
        samtools flagstat $TMPDIR/{wildcards.id}.filtMT.in.bam > {output.log_Mt}
        
        echo "index and filter reads mapped to blacklist"

        samtools view -hb -L {config[blacklist]} -U $TMPDIR/{wildcards.id}.filt_Mt_blcklist.bam -o $TMPDIR/{wildcards.id}.filt_Mt_blcklist.in.bam $TMPDIR/{wildcards.id}.filtMT.bam
        
        echo "compute stats for filtered bam file"

        samtools index $TMPDIR/{wildcards.id}.filt_Mt_blcklist.bam
        samtools index $TMPDIR/{wildcards.id}.filt_Mt_blcklist.in.bam
        samtools flagstat $TMPDIR/{wildcards.id}.filt_Mt_blcklist.in.bam > {output.log_blcklst}

    
        cp $TMPDIR/{wildcards.id}.filt_Mt_blcklist.bam {output.nonduprm}
        cp $TMPDIR/{wildcards.id}.filt_Mt_blcklist.bam.bai {output.nonduprm_idx}
        
        echo "filter duplicate reads and index final bam file"

        # rm dups
        samtools view -h -F 1024 $TMPDIR/{wildcards.id}.filt_Mt_blcklist.bam | samtools sort -T $TMPDIR/{wildcards.id}.sort.tmp -o {output.filt} -
    
        samtools index {output.filt}
        """

rule bam_stats:
    input:
        filtbamdir + "{id}.filt.bam"
    output:
        log_st_stats= filtbamdirstats + "{id}.filt.stats",
        log_st_flagstat= filtbamdirstats + "{id}.filt.flagstat",
        log_st_idxstats= filtbamdirstats + "{id}.filt.idxstats"
    shell:
        """
        module load bioinfo-tools
        module load samtools/1.9

        samtools flagstat {input} > {output.log_st_flagstat}
        samtools idxstats {input} > {output.log_st_idxstats}
        samtools stats {input} > {output.log_st_stats}
        """


################################
## final rules
################################

rule generate_rulegraph:
    """
    Generate a rulegraph for the workflow.
    """
    output:
        resdir + "rulegraph.png",
        resdir + "dag.png"
    shell:
        """
        snakemake --snakefile {config[snkpth]} --config max_reads=0 --rulegraph | dot -Tpng >{output[0]}
        snakemake --snakefile {config[snkpth]} --config max_reads=0 --dag | dot -Tpng >{output[1]}
        """

